# -*- coding: utf-8 -*-
"""Songketa.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pnFvTVHFa5NL3dprfeNuqvm20iZPhvd9
"""

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/My Drive/klasifikasi_songket

! ls

"""Read Random Data"""

from PIL import Image
import os
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import random

def view_random_image(target_dir, target_class):
  target_folder = target_dir + target_class
  random_image = random.sample(os.listdir(target_folder), 1)

  img = mpimg.imread(target_folder + "/" + random_image[0])
  plt.imshow(img)
  plt.title(target_class)
  plt.axis("off")

  print(f"Ukuran gambar:{img.shape}")
  return img

img = view_random_image("dataset_split/train/", "pucukRebung_riau")

"""Data Preprocessing"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale=1./255)

validation_datagen = ImageDataGenerator(rescale=1./255)

test_datagen = ImageDataGenerator(rescale=1./255)

base_dir = '/content/gdrive/My Drive/klasifikasi_songket/dataset_split/'
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')
test_dir = os.path.join(base_dir, 'test')

train_generator = train_datagen.flow_from_directory(train_dir,
                                                    target_size=(224,224),
                                                    class_mode='categorical',
                                                    batch_size=20)

validation_generator = validation_datagen.flow_from_directory(validation_dir,
                                                    target_size=(224,224),
                                                    class_mode='categorical',
                                                    batch_size=20)

test_generator = test_datagen.flow_from_directory(test_dir,
                                                  target_size=(224,224),
                                                  class_mode='categorical',
                                                  batch_size=20)

classes=list(train_generator.class_indices.keys())
print (classes)
class_count=len(classes)

"""Build Model"""

#model = tf.keras.models.Sequential([
#    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(200, 200, 3)),
#    tf.keras.layers.MaxPooling2D(2, 2),
#    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
#    tf.keras.layers.MaxPooling2D(2,2),
#    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
#    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
#    tf.keras.layers.MaxPooling2D(2,2),
#    tf.keras.layers.Dropout(0.5),
#    tf.keras.layers.Dense(256, activation='relu'),
#    tf.keras.layers.Dropout(0.5),
#    tf.keras.layers.Flatten(),
#    tf.keras.layers.Dense(4, activation='softmax')
#])

base_model=tf.keras.applications.mobilenet.MobileNet(include_top=False,
                                                     weights="imagenet",
                                                     input_shape=(224, 224, 3))

#base_model.summary()

base_model.trainable = False

model = tf.keras.models.Sequential([
            base_model,
            tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu', strides=1),
            tf.keras.layers.MaxPooling2D(2, 2),
            tf.keras.layers.Dropout(0.5),

            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(4, activation='softmax')
])


# Print the model summary
#model.summary()

optimizer = tf.keras.optimizers.Adam(learning_rate=.001)

model.compile(optimizer=optimizer,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(train_generator,
                    epochs=50,
                    validation_data=validation_generator,
                    verbose=1)

import matplotlib.pyplot as plt

# Plot the results
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend(loc=0)
plt.figure()

plt.plot(epochs, loss, 'r', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend(loc=0)
plt.figure()

plt.show()

"""Save Model"""

model.save("songketa_model.h5")

"""Predict Image"""

from tensorflow.keras.models import load_model
model = load_model('songketa_model.h5')

# Path ke gambar di Google Drive
img_path = '/content/sirangkak_sumateraBarat_12.jpg'

# Load dan preprocess gambar
img = image.load_img(img_path, target_size=(224, 224))
img_array = image.img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)
img_array /= 255.0

# Prediksi kelas
predictions = model.predict(img_array)
class_index = np.argmax(predictions, axis=1)

# Mapping indeks kelas ke label kelas
class_labels = {0: 'bunga_palembang', 1: 'pucukRebung_riau', 2: 'sirangkak_sumateraBarat', 3: 'subahnale_lombok'}
predicted_class = class_labels[class_index[0]]

# Print hasil prediksi
print(f'Predicted class: {predicted_class}')